{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 언어라는 현상을 모델링하고자 단어 시퀀스에 확률을 assign하는 모델 \n",
    "  \n",
    "+ **\"가장 자연스러운 단어 시퀀스(문장)를 찾아내는 모델\"**\n",
    "  \n",
    "+ 통계를 이용한 방법, 인공 신경망을 이용한 방법으로 나뉨  \n",
    "  \n",
    "+ 최근에는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여줌  \n",
    "  \n",
    "+ GPT, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 보편적으로는 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 방법이 많이 쓰임  \n",
    "  \n",
    "+ 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 방법도 쓰임 (빈칸 추론, BERT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 기계 번역  (Machine Translation)\n",
    "\n",
    "$ P(나는 \\, 버스를 \\, 탔다) > P(나는 \\, 버스를 \\, 태운다) $  \n",
    "\n",
    "\n",
    "#### b. 오타 교정 (Spell Correction)\n",
    "\n",
    "*선생님이 교실로 부리나케* $ P(달려갔다) > P(잘려갔다) $  \n",
    "\n",
    "#### c. 음성 인식 (Speech Recognition)\n",
    "\n",
    "$ P(나는 \\, 메롱을 \\, 먹는다) < P(나는 \\, 메론을 \\, 먹는다) $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 이전 단어들로부터 다음 단어 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. 단어 시퀀스의 확률  \n",
    "$ P(W) = P(w_1 \\, , \\, w_2 \\, , \\, ... \\, , \\, w_n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. 다음 단어 등장 확률  \n",
    "$ P(w_n \\, | \\, w_1 \\, , ... \\, , \\, w_{n-1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\therefore P(W) = \\prod_{i=1} ^ {n} P(W_n \\, | \\, w_1 \\, , \\, ... \\, , \\, w_{n-1})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
